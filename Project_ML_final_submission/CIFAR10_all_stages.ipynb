{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART1: CIFAR 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import average_precision_score\n",
    "from __future__ import print_function\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import load_model #save and load models\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import pickle\n",
    "from keras import regularizers\n",
    "\n",
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STAGE 1 starts here\n",
    "\n",
    "# The data, split between train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "#combining all the available data to use it in a way we want\n",
    "x = np.vstack((X_train,X_test))\n",
    "y = np.vstack((Y_train,Y_test))\n",
    "print('x shape:', x.shape)\n",
    "print('y shape:', y.shape)\n",
    "\n",
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "x = x.astype('float32')\n",
    "x = x/255\n",
    "num_classes = 10\n",
    "y1 = keras.utils.to_categorical(y, num_classes)\n",
    "print('y1 shape', y1.shape)\n",
    "print('Number of classes:', y1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters for target and shadow models\n",
    "batch_size = 32 #upto us\n",
    "epochs = 100\n",
    "lrate = 0.001\n",
    "decay = 1e-7 #find out what this decay parameter does\n",
    "kernel_size = (5,5) #upto us\n",
    "kernel_size2 = (3,3) #upto us\n",
    "nout1 = 32 #upto us\n",
    "nout2 = 32 #upto us\n",
    "ndense = 128\n",
    "#initializer in each layer - upto us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = [2500,5000,10000,15000]\n",
    "target_rep = np.zeros((len(data_size),x.shape[0]))\n",
    "ns = 10 #number of shadow models for one data_size\n",
    "\n",
    "for i,ds in enumerate(data_size): \n",
    "    sh = np.arange(x.shape[0])\n",
    "    np.random.shuffle(sh)\n",
    "    target_rep[i,:] = sh\n",
    "    xtr_target = x[sh[:ds]]\n",
    "    ytr_target = y1[sh[:ds]]\n",
    "    xts_target = x[sh[ds:2*ds]]\n",
    "    yts_target = y1[sh[ds:2*ds]]\n",
    "    shadow_rep = np.zeros((ns,x.shape[0]-2*ds))\n",
    "    sh2 = sh[2*ds:]\n",
    "    \n",
    "    #Training the target model when size of train & test data = ds\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(nout1, kernel_size, \n",
    "                     padding='valid', \n",
    "                     input_shape=xtr_target.shape[1:],\n",
    "                     activation='tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(nout2, kernel_size2, padding='valid', activation='tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(ndense, activation='tanh'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # initiate Adam optimizer\n",
    "    opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "\n",
    "    # Let's train the model using Adam\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    #print model summary just once\n",
    "    if i == 0:\n",
    "        print('Target model summary')\n",
    "        print(model.summary())\n",
    "    # Fit the model\n",
    "    #put verbose = 0 when actually running\n",
    "    hist_target = model.fit(xtr_target, ytr_target,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(xts_target, yts_target),\n",
    "                  shuffle=True,verbose=0)\n",
    "    print('\\n\\nFor target model with ds = %d'%ds)\n",
    "    print('Training accuracy = %f'%hist_target.history['acc'][-1])\n",
    "    print('Validation accuracy = %f'%hist_target.history['val_acc'][-1])\n",
    "    model_name = 'cifar10_target_'+str(ds)+'.h5'\n",
    "    model.save(model_name)\n",
    "    ytemp1 = model.predict(xtr_target)\n",
    "    ytemp2 = model.predict(xts_target)\n",
    "    xts_att = np.vstack((ytemp1,ytemp2))\n",
    "    yts_att = np.zeros(2*ds)\n",
    "    yts_att[:ds] = 1 \n",
    "    xts_att_truelabels = np.vstack((ytr_target,yts_target))\n",
    "    xts_att_dict = {'xts_att':xts_att,'yts_att':yts_att,'xts_att_truelabels':xts_att_truelabels}\n",
    "    fname = './att_test_data_'+str(ds)\n",
    "    np.save(fname,xts_att_dict)\n",
    "    \n",
    "    xtr_att = np.zeros((2*ds*ns,num_classes))\n",
    "    ytr_att = np.zeros((2*ds*ns,))\n",
    "    xtr_att_truelabels = np.zeros((2*ds*ns,num_classes))\n",
    "    for j in np.arange(ns):\n",
    "        np.random.shuffle(sh2)\n",
    "        shadow_rep[j,:] = sh2\n",
    "        xtr_sh1 = x[sh2[:ds]]\n",
    "        ytr_sh1 = y1[sh2[:ds]]\n",
    "        xts_sh1 = x[sh2[ds:2*ds]]\n",
    "        yts_sh1 = y1[sh2[ds:2*ds]]\n",
    "        \n",
    "        model_sh1 = Sequential()\n",
    "        model_sh1.add(Conv2D(nout1, kernel_size, \n",
    "                         padding='valid', \n",
    "                         input_shape=xtr_sh1.shape[1:],\n",
    "                         activation='tanh'))\n",
    "        model_sh1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model_sh1.add(Conv2D(nout2, kernel_size2, padding='valid', activation='tanh'))\n",
    "        model_sh1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model_sh1.add(Flatten())\n",
    "        model_sh1.add(Dense(ndense, activation='tanh'))\n",
    "        model_sh1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        # initiate Adam optimizer\n",
    "        opt_sh1 = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "\n",
    "        # Let's train the model using Adam\n",
    "        model_sh1.compile(loss='categorical_crossentropy',\n",
    "                          optimizer=opt_sh1,\n",
    "                          metrics=['accuracy'])\n",
    "        if j == 0 and i==0:\n",
    "            print('Shadow model summary:')\n",
    "            print(model_sh1.summary())\n",
    "        hist_sh1 = model_sh1.fit(xtr_sh1, ytr_sh1,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(xts_sh1, yts_sh1),\n",
    "                  shuffle=True,verbose=0)\n",
    "        model_name = 'cifar10_shadow_'+str(ds)+'_'+str(j)+'.h5'\n",
    "        model_sh1.save(model_name)\n",
    "        print('\\nFor shadow model %d'%j)\n",
    "        print('Training accuracy = %f'%hist_sh1.history['acc'][-1])\n",
    "        print('Validation accuracy = %f'%hist_sh1.history['val_acc'][-1])\n",
    "        ytemp11 = model_sh1.predict(xtr_sh1)\n",
    "        ytemp22 = model_sh1.predict(xts_sh1)\n",
    "        xtr_att[j*2*ds:(j+1)*2*ds] = np.vstack((ytemp11,ytemp22))\n",
    "        ytr_att[j*2*ds:(2*j+1)*ds] = 1\n",
    "        xtr_att_truelabels[j*2*ds:(j+1)*2*ds] = np.vstack((ytr_sh1,yts_sh1))\n",
    "    \n",
    "    #in outer for loop now\n",
    "    datafile = './data_cifar10_shadow_'+str(ds)\n",
    "    np.save(datafile,shadow_rep)\n",
    "    xtr_att_dict = {'xtr_att':xtr_att,'ytr_att':ytr_att,'xtr_att_truelabels':xtr_att_truelabels}\n",
    "    fname = './att_train_data_'+str(ds)\n",
    "    np.save(fname,xtr_att_dict)\n",
    "#outside both for loops\n",
    "np.save('./data_cifar10_target',target_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STAGE 2 starts here\n",
    "def getfilename(datatype,ds):\n",
    "    name = 'att_'+datatype+'_data_'+str(ds)+'.npy'\n",
    "    return name\n",
    "def getmodelname(classi,ds):\n",
    "    name = 'att_model_'+str(ds)+'_class_'+str(classi)+'.p'\n",
    "    return name\n",
    "\n",
    "\n",
    "C_test = [0.1,1,10]\n",
    "gam_test = [0.001,0.01,0.1]\n",
    "bestCgam = []\n",
    "for ds in data_size:\n",
    "    xtr_list = []\n",
    "    ytr_list = []\n",
    "    xts_list = []\n",
    "    yts_list = []\n",
    "    \n",
    "    train_dict = np.load(getfilename('train',ds)).item()\n",
    "    test_dict = np.load(getfilename('test',ds)).item()\n",
    "    \n",
    "    xtr_att_truelabels = train_dict['xtr_att_truelabels']\n",
    "    xtr_att = train_dict['xtr_att']\n",
    "    ytr_att = train_dict['ytr_att']\n",
    "    \n",
    "    xts_att_truelabels = test_dict['xts_att_truelabels']\n",
    "    xts_att = test_dict['xts_att']\n",
    "    yts_att = test_dict['yts_att']\n",
    "    \n",
    "    for i in np.arange(10):\n",
    "        ind = np.where(xtr_att_truelabels[:,i]==1)[0]\n",
    "        xtr_list.append(xtr_att[ind])\n",
    "        ytr_list.append(ytr_att[ind])\n",
    "        \n",
    "        ind2 = np.where(xts_att_truelabels[:,i]==1)[0]\n",
    "        xts_list.append(xts_att[ind2])\n",
    "        yts_list.append(yts_att[ind2])\n",
    "    \n",
    "    class_c_g = np.zeros((10,2))\n",
    "    prec = []\n",
    "    recall = []\n",
    "    acc_classwise = []\n",
    "    yhat_full = np.zeros(2*ds)\n",
    "    y_full = np.zeros(2*ds)\n",
    "    start = 0\n",
    "    for i in np.arange(10):\n",
    "        x = xtr_list[i]\n",
    "        y = ytr_list[i]\n",
    "        \n",
    "        xtest = xts_list[i]\n",
    "        ytest = yts_list[i]\n",
    "        ntest = xtest.shape[0]\n",
    "        nfold = 5\n",
    "        kf = KFold(n_splits=nfold, shuffle=True)\n",
    "        acc = np.zeros((3,3,nfold))\n",
    "        for ifold, ind in enumerate(kf.split(x)):\n",
    "            # Get the training data in the split\n",
    "            Itr,Its = ind\n",
    "            xtr = x[Itr,:]\n",
    "            ytr = y[Itr]\n",
    "            xts = x[Its,:]\n",
    "            yts = y[Its]\n",
    "            for ic,c in enumerate(C_test):\n",
    "                for ig,g in enumerate(gam_test):\n",
    "                    svc = svm.SVC(probability=False,  kernel=\"rbf\", C=c, gamma=g,verbose=10)\n",
    "                    svc.fit(xtr,ytr)\n",
    "                    yhat_ts = svc.predict(xts)\n",
    "                    acc[ic,ig,ifold] = np.mean(yhat_ts == yts)\n",
    "        \n",
    "        acc1 = np.mean(acc,axis=2)\n",
    "        ci = np.argmax(np.amax(acc1,axis=1))\n",
    "        gi = np.argmax(np.amax(acc1,axis=0))\n",
    "        class_c_g[i,0] = C_test[ci]\n",
    "        class_c_g[i,1] = gam_test[gi]\n",
    "        \n",
    "        #Now creating the actual attack classifier for class i and data_size ds\n",
    "        svc = svm.SVC(probability=False,  kernel=\"rbf\", C=C_test[ci], gamma=gam_test[gi], verbose=10)\n",
    "        svc.fit(x,y)\n",
    "        yhat_test = svc.predict(xtest)\n",
    "        acci = np.mean(yhat_test == ytest)\n",
    "        preci,reci,_,_= precision_recall_fscore_support(ytest,yhat_test,average='binary')\n",
    "        prec.append(preci)\n",
    "        recall.append(reci)\n",
    "        acc_classwise.append(acci)\n",
    "        modelname = getmodelname(i,ds)\n",
    "        with open( modelname, \"wb\" ) as fp:\n",
    "            pickle.dump( [svc, C_test[ci], gam_test[gi]], fp)\n",
    "        \n",
    "        y_full[start:(start+ntest)] = ytest\n",
    "        yhat_full[start:(start+ntest)] = yhat_test\n",
    "        \n",
    "        start = start+ntest\n",
    "    \n",
    "    acctotal = np.mean(yhat_full == y_full)\n",
    "    prectotal,recalltotal,_,_= precision_recall_fscore_support(y_full,yhat_full,average='binary')\n",
    "    fname = getfilename('results',ds)\n",
    "    dict_to_save = {'prectotal':prectotal,\n",
    "                   'acctotal':acctotal,\n",
    "                   'recalltotal':recalltotal,\n",
    "                   'prec':prec,\n",
    "                   'acc':acc_classwise,\n",
    "                   'recall':recall,\n",
    "                   'class_c_g':class_c_g}\n",
    "    np.save(fname,dict_to_save)\n",
    "    print('For ds = ',ds)\n",
    "    print('Attack Precision: ',prectotal)\n",
    "    print('Attack Recall: ',recalltotal)\n",
    "    print('Attack Accuracy: ',acctotal)\n",
    "    \n",
    "    \n",
    "                    \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = []\n",
    "rec = []\n",
    "acc = []\n",
    "\n",
    "dict1 = np.load(getfilename('results',2500)).item()\n",
    "dict2 = np.load(getfilename('results',5000)).item()\n",
    "dict3 = np.load(getfilename('results',10000)).item()\n",
    "dict4 = np.load(getfilename('results',15000)).item()\n",
    "\n",
    "prec.append(dict1['prectotal'])\n",
    "prec.append(dict2['prectotal'])\n",
    "prec.append(dict3['prectotal'])\n",
    "prec.append(dict4['prectotal'])\n",
    "\n",
    "rec.append(dict1['recalltotal'])\n",
    "rec.append(dict2['recalltotal'])\n",
    "rec.append(dict3['recalltotal'])\n",
    "rec.append(dict4['recalltotal'])\n",
    "\n",
    "acc.append(dict1['acctotal'])\n",
    "acc.append(dict2['acctotal'])\n",
    "acc.append(dict3['acctotal'])\n",
    "acc.append(dict4['acctotal'])\n",
    "\n",
    "prec1 = dict1['prec']\n",
    "prec2 = dict2['prec']\n",
    "prec3 = dict3['prec']\n",
    "prec4 = dict4['prec']\n",
    "\n",
    "rec1 = dict1['recall']\n",
    "rec2 = dict2['recall']\n",
    "rec3 = dict3['recall']\n",
    "rec4 = dict4['recall']\n",
    "\n",
    "acc1 = dict1['acc']\n",
    "acc2 = dict2['acc']\n",
    "acc3 = dict3['acc']\n",
    "acc4 = dict4['acc']\n",
    "\n",
    "cgsvm1 = dict1['class_c_g']\n",
    "cgsvm2 = dict2['class_c_g']\n",
    "cgsvm3 = dict3['class_c_g']\n",
    "cgsvm4 = dict4['class_c_g']\n",
    "\n",
    "baseline = 0.5*np.ones(10)\n",
    "plt.plot(prec1)\n",
    "plt.plot(prec2)\n",
    "plt.plot(prec3)\n",
    "plt.plot(prec4)\n",
    "plt.plot(baseline)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(['2500','5000','10000','15000','baseline'],loc='lower right')\n",
    "plt.title('CIFAR-10, CNN , Membership Iinference Attack')\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(data_size,prec)\n",
    "\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.title('CIFAR-10, CNN , Membership Iinference Attack')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MITIGATION PART\n",
    "reg = 5e-3\n",
    "print('Using L2 regularization with regularization constant = ',reg)\n",
    "data_size = [2500,5000,15000]\n",
    "target_rep = np.load('data_cifar10_target.npy')\n",
    "for i,ds in enumerate(data_size):\n",
    "    sh = target_rep[i,:].astype(int) \n",
    "    xtr_target = x[sh[:ds]]\n",
    "    ytr_target = y1[sh[:ds]]\n",
    "    xts_target = x[sh[ds:2*ds]]\n",
    "    yts_target = y1[sh[ds:2*ds]]\n",
    "    \n",
    "    #Training the target model when size of train & test data = ds\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(nout1, kernel_size, \n",
    "                     padding='valid', \n",
    "                     input_shape=xtr_target.shape[1:],\n",
    "                     activation='tanh',bias_regularizer=regularizers.l2(reg),kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(nout2, kernel_size2, padding='valid', activation='tanh',bias_regularizer=regularizers.l2(reg),\n",
    "                     kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(ndense, activation='tanh',bias_regularizer=regularizers.l2(reg),\n",
    "                    kernel_regularizer=regularizers.l2(reg)))\n",
    "    model.add(Dense(num_classes, activation='softmax',bias_regularizer=regularizers.l2(reg),\n",
    "                    kernel_regularizer=regularizers.l2(reg)))\n",
    "\n",
    "    # initiate Adam optimizer\n",
    "    opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "\n",
    "    # Let's train the model using Adam\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    #print model summary just once\n",
    "    if i == 0:\n",
    "        print('Target model summary')\n",
    "        print(model.summary())\n",
    "    # Fit the model\n",
    "    #put verbose = 0 when actually running\n",
    "    hist_target = model.fit(xtr_target, ytr_target,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  validation_data=(xts_target, yts_target),\n",
    "                  shuffle=True,verbose=0)\n",
    "    print('\\n\\nFor target model with ds = %d'%ds)\n",
    "    print('Training accuracy = %f'%hist_target.history['acc'][-1])\n",
    "    print('Validation accuracy = %f'%hist_target.history['val_acc'][-1])\n",
    "    model_name = 'cifar10_target_after_mitigation_'+str(ds)+'.h5'\n",
    "    model.save(model_name)\n",
    "    ytemp1 = model.predict(xtr_target)\n",
    "    ytemp2 = model.predict(xts_target)\n",
    "    xts_att = np.vstack((ytemp1,ytemp2))\n",
    "    yts_att = np.zeros(2*ds)\n",
    "    yts_att[:ds] = 1 \n",
    "    xts_att_truelabels = np.vstack((ytr_target,yts_target))\n",
    "    xts_att_dict = {'xts_att':xts_att,'yts_att':yts_att,'xts_att_truelabels':xts_att_truelabels}\n",
    "    fname = './att_test_data_after_mitigation_'+str(ds)\n",
    "    np.save(fname,xts_att_dict)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ATTACK RESULTS AFTER MITIGATION EMPLOYED IN TARGET MODEL\n",
    "for ds in data_size:\n",
    "    xtr_list = []\n",
    "    ytr_list = []\n",
    "    xts_list = []\n",
    "    yts_list = []\n",
    "    \n",
    "    train_dict = np.load(getfilename('train',ds)).item()\n",
    "    test_dict = np.load(getfilename('test',ds)).item()\n",
    "    \n",
    "    xtr_att_truelabels = train_dict['xtr_att_truelabels']\n",
    "    xtr_att = train_dict['xtr_att']\n",
    "    ytr_att = train_dict['ytr_att']\n",
    "    \n",
    "    xts_att_truelabels = test_dict['xts_att_truelabels']\n",
    "    xts_att = test_dict['xts_att']\n",
    "    yts_att = test_dict['yts_att']\n",
    "    \n",
    "    for i in np.arange(10):\n",
    "        ind = np.where(xtr_att_truelabels[:,i]==1)[0]\n",
    "        xtr_list.append(xtr_att[ind])\n",
    "        ytr_list.append(ytr_att[ind])\n",
    "        \n",
    "        ind2 = np.where(xts_att_truelabels[:,i]==1)[0]\n",
    "        xts_list.append(xts_att[ind2])\n",
    "        yts_list.append(yts_att[ind2])\n",
    "    \n",
    "    class_c_g = np.zeros((10,2))\n",
    "    prec = []\n",
    "    recall = []\n",
    "    acc_classwise = []\n",
    "    yhat_full = np.zeros(2*ds)\n",
    "    y_full = np.zeros(2*ds)\n",
    "    start = 0\n",
    "    for i in np.arange(10):\n",
    "        x = xtr_list[i]\n",
    "        y = ytr_list[i]\n",
    "        \n",
    "        xtest = xts_list[i]\n",
    "        ytest = yts_list[i]\n",
    "        ntest = xtest.shape[0]\n",
    "        nfold = 5\n",
    "        kf = KFold(n_splits=nfold, shuffle=True)\n",
    "        acc = np.zeros((3,3,nfold))\n",
    "        for ifold, ind in enumerate(kf.split(x)):\n",
    "            # Get the training data in the split\n",
    "            Itr,Its = ind\n",
    "            xtr = x[Itr,:]\n",
    "            ytr = y[Itr]\n",
    "            xts = x[Its,:]\n",
    "            yts = y[Its]\n",
    "            for ic,c in enumerate(C_test):\n",
    "                for ig,g in enumerate(gam_test):\n",
    "                    svc = svm.SVC(probability=False,  kernel=\"rbf\", C=c, gamma=g,verbose=10)\n",
    "                    svc.fit(xtr,ytr)\n",
    "                    yhat_ts = svc.predict(xts)\n",
    "                    acc[ic,ig,ifold] = np.mean(yhat_ts == yts)\n",
    "        \n",
    "        acc1 = np.mean(acc,axis=2)\n",
    "        ci = np.argmax(np.amax(acc1,axis=1))\n",
    "        gi = np.argmax(np.amax(acc1,axis=0))\n",
    "        class_c_g[i,0] = C_test[ci]\n",
    "        class_c_g[i,1] = gam_test[gi]\n",
    "        \n",
    "        #Now creating the actual attack classifier for class i and data_size ds\n",
    "        svc = svm.SVC(probability=False,  kernel=\"rbf\", C=C_test[ci], gamma=gam_test[gi], verbose=10)\n",
    "        svc.fit(x,y)\n",
    "        yhat_test = svc.predict(xtest)\n",
    "        acci = np.mean(yhat_test == ytest)\n",
    "        preci,reci,_,_= precision_recall_fscore_support(ytest,yhat_test,average='binary')\n",
    "        prec.append(preci)\n",
    "        recall.append(reci)\n",
    "        acc_classwise.append(acci)\n",
    "        modelname = getmodelname(i,ds)\n",
    "        with open( modelname, \"wb\" ) as fp:\n",
    "            pickle.dump( [svc, C_test[ci], gam_test[gi]], fp)\n",
    "        \n",
    "        y_full[start:(start+ntest)] = ytest\n",
    "        yhat_full[start:(start+ntest)] = yhat_test\n",
    "        \n",
    "        start = start+ntest\n",
    "    \n",
    "    acctotal = np.mean(yhat_full == y_full)\n",
    "    prectotal,recalltotal,_,_= precision_recall_fscore_support(y_full,yhat_full,average='binary')\n",
    "    fname = getfilename('results',ds)\n",
    "    dict_to_save = {'prectotal':prectotal,\n",
    "                   'acctotal':acctotal,\n",
    "                   'recalltotal':recalltotal,\n",
    "                   'prec':prec,\n",
    "                   'acc':acc_classwise,\n",
    "                   'recall':recall,\n",
    "                   'class_c_g':class_c_g}\n",
    "    np.save(fname,dict_to_save)\n",
    "    print('For ds = ',ds)\n",
    "    print('Attack Precision: ',prectotal)\n",
    "    print('Attack Recall: ',recalltotal)\n",
    "    print('Attack Accuracy: ',acctotal)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART2: UCI ADULT (CENSUS INCOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"adult census/adult.data\", names=[\n",
    "        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
    "        \"Hours per week\", \"Country\", \"Target\"],\n",
    "        sep=r'\\s*,\\s*',\n",
    "        engine='python',\n",
    "        na_values=\"?\")\n",
    "df1 = pd.read_csv(\n",
    "    \"adult census/adult.test\",\n",
    "    names=[\n",
    "        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
    "        \"Hours per week\", \"Country\", \"Target\"],\n",
    "        sep=r'\\s*,\\s*',\n",
    "        engine='python',\n",
    "        na_values=\"?\")\n",
    "data = pd.concat([df, df1], ignore_index= True)\n",
    "print(data.shape)\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_cloud = data.columns.tolist()\n",
    "print(names_cloud)\n",
    "X = np.array(data[names_cloud])\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "cols = 5\n",
    "rows = (float(data.shape[1]) / cols)\n",
    "for i, column in enumerate(data.columns):\n",
    "    a = fig.add_subplot(rows, cols, i + 1)\n",
    "    a.set_title(column)\n",
    "    if data.dtypes[column] == np.object:\n",
    "        data[column].value_counts().plot(kind=\"bar\", axes=a)\n",
    "    else:\n",
    "        data[column].hist(axes=a)\n",
    "        plt.xticks(rotation=\"vertical\")\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (data['Target'].map({\"<=50K\":0,\">50K\":1})).values\n",
    "print(pd.value_counts(pd.Series(y)))\n",
    "data.drop('Target',axis=1, inplace =True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = data.select_dtypes(include=['object']).columns\n",
    "print(categorical_features)\n",
    "ohc_category = ['Workclass', 'Education', 'Martial Status', 'Occupation', 'Relationship', 'Race', 'Sex', 'Country']\n",
    "df_ohc = pd.get_dummies(data, columns = ohc_category)\n",
    "print(df_ohc.shape)\n",
    "df_ohc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_x = df_ohc.columns.tolist()\n",
    "print(\"Target Variable: Target\")\n",
    "print(\"Predictors: \"+str(names_x))\n",
    "x = np.array(df_ohc[names_x])\n",
    "print(\"Number of data samples : {0:d}\".format(x.shape[0]))\n",
    "print(\"Number of Predictor Features : {0:d}\".format(x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.astype('float32')\n",
    "x = x/255\n",
    "batch_size = 32 #upto us\n",
    "epochs = 100\n",
    "lrate = 0.001\n",
    "decay = 1e-7 \n",
    "data_size = 10000\n",
    "ns = 20 #number of shadow models for one data_size\n",
    "nh = 5 #number of hidden layers\n",
    "nout = 1\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "sh = np.arange(x.shape[0])\n",
    "np.random.shuffle(sh)\n",
    "target_rep = np.zeros((1,x.shape[0]))\n",
    "target_rep[0,:] = sh\n",
    "print(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.clear_session()\n",
    "xtr_target = x[sh[:data_size]]\n",
    "ytr_target = y[sh[:data_size]]\n",
    "xts_target = x[sh[data_size:data_size*2]]\n",
    "yts_target = y[sh[data_size:2*data_size]]\n",
    "shadow_rep = np.zeros((20,x.shape[0]-2*data_size))\n",
    "sh1 = sh[2*data_size:]\n",
    "xtr_att = np.zeros((2*data_size*ns,1))\n",
    "ytr_att = np.zeros((2*data_size*ns,1))\n",
    "xtr_att_truelabels = np.zeros((2*data_size*ns,))\n",
    "model_target = Sequential()\n",
    "model_target.add(Dense(nh, input_shape =(x.shape[1],), activation='sigmoid', name = 'hidden'))\n",
    "model_target.add(Dense(1, activation='sigmoid', name = 'output'))\n",
    "opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "model_target.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "print(model_target.summary())\n",
    "hist_target = model_target.fit(xtr_target, ytr_target,\n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,\n",
    "                  validation_data=(xts_target, yts_target), shuffle=True, verbose=0)\n",
    "print('\\n\\nFor target model with training datasize = %d'%data_size)\n",
    "print('Training accuracy = %f'%hist_target.history['acc'][-1])\n",
    "print('Validation accuracy = %f'%hist_target.history['val_acc'][-1])\n",
    "model_target_name = 'UCI_Adult_target_'+str(data_size)+'.h5'\n",
    "model_target.save(model_target_name)\n",
    "ytemp_tr_target = model_target.predict(xtr_target)\n",
    "ytemp_ts_target = model_target.predict(xts_target)\n",
    "xts_att = np.vstack((ytemp_tr_target,ytemp_ts_target))\n",
    "yts_att = np.zeros((2*data_size,1))\n",
    "yts_att[data_size:2*data_size] = 1  \n",
    "xts_att_truelabels = np.vstack((ytr_target,yts_target))\n",
    "xts_att_dict = {'xts_att':xts_att,'yts_att':yts_att,'xts_att_truelabels':xts_att_truelabels}\n",
    "fname = './att_test_data_'+str(data_size)\n",
    "np.save(fname,xts_att_dict)\n",
    "datafile = './data_adult_target_'+str(data_size)\n",
    "np.save(datafile,target_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(ns):\n",
    "    np.random.shuffle(sh1)\n",
    "    shadow_rep[i,:] = sh1\n",
    "    xtr_shadow = x[sh1[:data_size]]\n",
    "    ytr_shadow = y[sh1[:data_size]]\n",
    "    xts_shadow = x[sh1[data_size:2*data_size]]\n",
    "    yts_shadow = y[sh1[data_size:2*data_size]]\n",
    "    model_shadow = Sequential()\n",
    "    model_shadow.add(Dense(nh, input_shape =(x.shape[1],), activation='sigmoid', name = 'hidden'))\n",
    "    model_shadow.add(Dense(1, activation='sigmoid', name = 'output'))\n",
    "    opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "    model_shadow.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    if i == 0:\n",
    "        print(\"Shadow Model Summary\")\n",
    "        print(model_shadow.summary())\n",
    "    hist_shadow = model_shadow.fit(xtr_shadow, ytr_shadow,\n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,\n",
    "                  validation_data=(xts_shadow, yts_shadow), shuffle=True, verbose=0)\n",
    "    print(\"Shadow model no: %d\"%i)\n",
    "    print('\\n\\nFor shadow model with training datasize = %d'%data_size)\n",
    "    print('Training accuracy = %f'%hist_shadow.history['acc'][-1])\n",
    "    print('Validation accuracy = %f'%hist_shadow.history['val_acc'][-1])\n",
    "    ytemp11 = model_shadow.predict(xtr_shadow)\n",
    "    ytemp22 = model_shadow.predict(xts_shadow)\n",
    "    model_shadow_name = 'UCI_Adult_shadow_'+str(data_size)+'_'+str(i)+'.h5'\n",
    "    print(model_shadow_name)\n",
    "    model_shadow.save(model_shadow_name)    \n",
    "    xtr_att[i*2*data_size:(i+1)*2*data_size] = np.vstack((ytemp11,ytemp22))\n",
    "    ytr_att[((i*2)+1)*data_size:(i+1)*2*data_size] = 1\n",
    "    xtr_att_truelabels[i*2*data_size:(i+1)*2*data_size] = np.hstack((ytr_shadow,yts_shadow))\n",
    "datafile = './data_adult_shadow_'+str(data_size)\n",
    "np.save(datafile,shadow_rep)\n",
    "xtr_att_dict = {'xtr_att':xtr_att,'ytr_att':ytr_att,'xtr_att_truelabels':xtr_att_truelabels}\n",
    "fname = './att_train_data_'+str(data_size)\n",
    "np.save(fname,xtr_att_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_attack = Sequential()\n",
    "model_attack.add(Dense(nh, input_shape = (xtr_att.shape[1],), activation='sigmoid', name = 'hidden'))\n",
    "model_attack.add(Dense(1, activation='sigmoid', name = 'output'))\n",
    "opt = keras.optimizers.adam(lr = lrate, decay=decay)\n",
    "model_attack.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "print(\"Attack Model Summary\")\n",
    "print(model_attack.summary())\n",
    "hist_attack = model_attack.fit(xtr_att, ytr_att,\n",
    "                  batch_size = batch_size,\n",
    "                  epochs = epochs,\n",
    "                  validation_data=(xts_att, yts_att), shuffle=True, verbose=0)\n",
    "print('\\n\\nFor attack model with training datasize = %d'%xtr_att.shape[0])\n",
    "print('Training accuracy = %f'%hist_attack.history['acc'][-1])\n",
    "print('Validation accuracy = %f'%hist_attack.history['val_acc'][-1])\n",
    "y_score = model_attack.predict(xts_att)\n",
    "average_precision = average_precision_score(yts_att, y_score)\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mitigation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = [2500,5000,15000]\n",
    "num = 0\n",
    "for ds in data_size:\n",
    "    xtr_list = []\n",
    "    ytr_list = []\n",
    "    xts_list = []\n",
    "    yts_list = []\n",
    "    name = 'att_test_data_after_mitigation_' +str(ds)+'.npy'\n",
    "    test_dict = np.load(name,encoding='bytes').item()\n",
    "    \n",
    "    \n",
    "    xts_att_truelabels = test_dict[b'xts_att_truelabels']\n",
    "    xts_att = test_dict[b'xts_att']\n",
    "    yts_att = test_dict[b'yts_att']\n",
    "    \n",
    "    for i in np.arange(10):\n",
    "        ind2 = np.where(xts_att_truelabels[:,i]==1)[0]\n",
    "        xts_list.append(xts_att[ind2])\n",
    "        yts_list.append(yts_att[ind2])\n",
    "    \n",
    "    prec = []\n",
    "    recall = []\n",
    "    acc_classwise = []\n",
    "    yhat_full = np.zeros(2*ds)\n",
    "    y_full = np.zeros(2*ds)\n",
    "    start = 0\n",
    "    for i in np.arange(10):\n",
    "        xtest = xts_list[i]\n",
    "        ytest = yts_list[i]\n",
    "        ntest = xtest.shape[0]\n",
    "        name = 'att_model_'+str(ds)+'_class_'+str(i)+'.p'\n",
    "        with open( name, \"rb\" ) as fp:\n",
    "            svc, _,_ = pickle.load(fp)\n",
    "        yhat_test = svc.predict(xtest)\n",
    "        acci = np.mean(yhat_test == ytest)\n",
    "        preci,reci,_,_= precision_recall_fscore_support(ytest,yhat_test,average='binary')\n",
    "        prec.append(preci)\n",
    "        recall.append(reci)\n",
    "        acc_classwise.append(acci)\n",
    "        y_full[start:(start+ntest)] = ytest\n",
    "        yhat_full[start:(start+ntest)] = yhat_test\n",
    "        \n",
    "        start = start+ntest\n",
    "    \n",
    "    acctotal = np.mean(yhat_full == y_full)\n",
    "    prectotal,recalltotal,_,_= precision_recall_fscore_support(y_full,yhat_full,average='binary')\n",
    "    print('For ds = ',ds)\n",
    "    print('Attack Precision: ',prectotal)\n",
    "    print('Attack Recall: ',recalltotal)\n",
    "    print('Attack Accuracy: ',acctotal)\n",
    "    strname = 'Accuracy vs classes curve after applying mitigation'\n",
    "    \n",
    "    plt.plot(acc_classwise)\n",
    "    \n",
    "plt.title(strname)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['2500','5000','15000'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
